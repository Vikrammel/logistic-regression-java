iteration: 0 lik: -1974.6944134338808
iteration: 1 lik: -1385.0261894180192
iteration: 2 lik: -1180.192696177289
iteration: 3 lik: -1062.3311010805455
iteration: 4 lik: -982.2925343141981
iteration: 5 lik: -922.8726933806471
iteration: 6 lik: -876.2082581110775
iteration: 7 lik: -838.1142347072786
iteration: 8 lik: -806.1272040791237
iteration: 9 lik: -778.6883440232389
iteration: 10 lik: -754.7556504619223
iteration: 11 lik: -733.6017057362217
iteration: 12 lik: -714.7002886786362
iteration: 13 lik: -697.6590790587215
iteration: 14 lik: -682.1779022056724
iteration: 15 lik: -668.021906442921
iteration: 16 lik: -655.0038389373279
iteration: 17 lik: -642.9720260211241
iteration: 18 lik: -631.8020011376876
iteration: 19 lik: -621.3905073520576
iteration: 20 lik: -611.651081413692
iteration: 21 lik: -602.5107230731932
iteration: 22 lik: -593.9073342788625
iteration: 23 lik: -585.7877216610492
iteration: 24 lik: -578.1060213852707
iteration: 25 lik: -570.8224462964696
iteration: 26 lik: -563.9022819505717
iteration: 27 lik: -557.3150764942586
iteration: 28 lik: -551.033982554559
iteration: 29 lik: -545.0352190632774
iteration: 30 lik: -539.297628281749
iteration: 31 lik: -533.8023088584215
iteration: 32 lik: -528.5323099943387
iteration: 33 lik: -523.4723750354744
iteration: 34 lik: -518.6087252993095
iteration: 35 lik: -513.928876858707
iteration: 36 lik: -509.4214844871366
iteration: 37 lik: -505.0762081197901
iteration: 38 lik: -500.8835980836106
iteration: 39 lik: -496.83499605495626
iteration: 40 lik: -492.92244926137795
iteration: 41 lik: -489.13863588747995
iteration: 42 lik: -485.4767999996084
iteration: 43 lik: -481.93069459002
iteration: 44 lik: -478.49453157264435
iteration: 45 lik: -475.1629377512708
iteration: 46 lik: -471.93091593583756
iteration: 47 lik: -468.79381050991395
iteration: 48 lik: -465.74727685822324
iteration: 49 lik: -462.7872541510166
iteration: 50 lik: -459.90994105558354
iteration: 51 lik: -457.11177400699063
iteration: 52 lik: -454.3894077220739
iteration: 53 lik: -451.73969768468186
iteration: 54 lik: -449.15968436741156
iteration: 55 lik: -446.64657898668224
iteration: 56 lik: -444.1977506150383
iteration: 57 lik: -441.81071449752
iteration: 58 lik: -439.48312143868776
iteration: 59 lik: -437.21274814380985
iteration: 60 lik: -434.9974884121666
iteration: 61 lik: -432.8353450930544
iteration: 62 lik: -430.72442272577916
iteration: 63 lik: -428.6629207942683
iteration: 64 lik: -426.64912753514784
iteration: 65 lik: -424.68141424498924
iteration: 66 lik: -422.7582300387127
iteration: 67 lik: -420.8780970163833
iteration: 68 lik: -419.03960580038233
iteration: 69 lik: -417.24141140895983
iteration: 70 lik: -415.48222943590076
iteration: 71 lik: -413.76083250904986
iteration: 72 lik: -412.07604700340136
iteration: 73 lik: -410.42674998684583
iteration: 74 lik: -408.8118663788355
iteration: 75 lik: -407.2303663042946
iteration: 76 lik: -405.68126262670523
iteration: 77 lik: -404.16360864591223
iteration: 78 lik: -402.67649594754283
iteration: 79 lik: -401.21905239215835
iteration: 80 lik: -399.7904402333747
iteration: 81 lik: -398.3898543551315
iteration: 82 lik: -397.0165206192021
iteration: 83 lik: -395.66969431480777
iteration: 84 lik: -394.34865870291594
iteration: 85 lik: -393.05272364843586
iteration: 86 lik: -391.7812243340765
iteration: 87 lik: -390.53352005020287
iteration: 88 lik: -389.3089930554321
iteration: 89 lik: -388.10704750315864
iteration: 90 lik: -386.9271084295793
iteration: 91 lik: -385.76862079915753
iteration: 92 lik: -384.63104860369594
iteration: 93 lik: -383.51387401158144
iteration: 94 lik: -382.4165965639417
iteration: 95 lik: -381.33873241472884
iteration: 96 lik: -380.2798136119495
iteration: 97 lik: -379.23938741744814
iteration: 98 lik: -378.2170156628577
iteration: 99 lik: -377.21227413946593
iteration: 100 lik: -376.2247520199371
iteration: 101 lik: -375.2540513098996
iteration: 102 lik: -374.29978632766614
iteration: 103 lik: -373.3615832102983
iteration: 104 lik: -372.43907944450723
iteration: 105 lik: -371.5319234209119
iteration: 106 lik: -370.6397740102057
iteration: 107 lik: -369.76230016002825
iteration: 108 lik: -368.89918051128456
iteration: 109 lik: -368.0501030327898
iteration: 110 lik: -367.2147646731854
iteration: 111 lik: -366.39287102915983
iteration: 112 lik: -365.58413602899606
iteration: 113 lik: -364.7882816306427
iteration: 114 lik: -364.00503753345725
iteration: 115 lik: -363.23414090286303
iteration: 116 lik: -362.4753361072444
iteration: 117 lik: -361.7283744663669
iteration: 118 lik: -360.9930140107392
iteration: 119 lik: -360.2690192513084
iteration: 120 lik: -359.55616095895726
iteration: 121 lik: -358.8542159532814
iteration: 122 lik: -358.1629669001636
iteration: 123 lik: -357.4822021177184
iteration: 124 lik: -356.8117153901349
iteration: 125 lik: -356.1513057890852
iteration: 126 lik: -355.5007775022619
iteration: 127 lik: -354.85993966875293
iteration: 128 lik: -354.2286062208753
iteration: 129 lik: -353.6065957321783
iteration: 130 lik: -352.99373127134106
iteration: 131 lik: -352.3898402616582
iteration: 132 lik: -351.79475434586993
iteration: 133 lik: -351.20830925610625
iteration: 134 lik: -350.6303446886768
iteration: 135 lik: -350.0607041835285
iteration: 136 lik: -349.4992350081465
iteration: 137 lik: -348.9457880456905
iteration: 138 lik: -348.400217687209
iteration: 139 lik: -347.8623817277448
iteration: 140 lik: -347.33214126614814
iteration: 141 lik: -346.8093606084735
iteration: 142 lik: -346.2939071747802
iteration: 143 lik: -345.78565140921495
iteration: 144 lik: -345.2844666932286
iteration: 145 lik: -344.79022926181176
iteration: 146 lik: -344.3028181226011
iteration: 147 lik: -343.8221149777824
iteration: 148 lik: -343.34800414863287
iteration: 149 lik: -342.880372502631
iteration: 150 lik: -342.41910938301385
iteration: 151 lik: -341.96410654069314
iteration: 152 lik: -341.51525806844006
iteration: 153 lik: -341.072460337246
iteration: 154 lik: -340.6356119347747
iteration: 155 lik: -340.2046136058325
iteration: 156 lik: -339.7793681947637
iteration: 157 lik: -339.3597805897313
iteration: 158 lik: -338.9457576687649
iteration: 159 lik: -338.53720824755385
iteration: 160 lik: -338.13404302889603
iteration: 161 lik: -337.7361745537279
iteration: 162 lik: -337.343517153734
iteration: 163 lik: -336.95598690540976
iteration: 164 lik: -336.57350158555505
iteration: 165 lik: -336.1959806281671
iteration: 166 lik: -335.82334508263193
iteration: 167 lik: -335.45551757322215
iteration: 168 lik: -335.0924222598175
iteration: 169 lik: -334.7339847998131
iteration: 170 lik: -334.3801323111772
iteration: 171 lik: -334.0307933366209
iteration: 172 lik: -333.6858978088365
iteration: 173 lik: -333.3453770167579
iteration: 174 lik: -333.0091635728316
iteration: 175 lik: -332.6771913812285
iteration: 176 lik: -332.3493956070016
iteration: 177 lik: -332.02571264613584
iteration: 178 lik: -331.7060800964522
iteration: 179 lik: -331.39043672936145
iteration: 180 lik: -331.07872246242374
iteration: 181 lik: -330.77087833267336
iteration: 182 lik: -330.46684647071993
iteration: 183 lik: -330.166570075568
iteration: 184 lik: -329.8699933901412
iteration: 185 lik: -329.57706167748364
iteration: 186 lik: -329.2877211976413
iteration: 187 lik: -329.0019191851535
iteration: 188 lik: -328.71960382717896
iteration: 189 lik: -328.4407242422262
iteration: 190 lik: -328.16523045943484
iteration: 191 lik: -327.8930733984384
iteration: 192 lik: -327.62420484977025
iteration: 193 lik: -327.3585774557827
iteration: 194 lik: -327.0961446920711
iteration: 195 lik: -326.83686084940877
iteration: 196 lik: -326.5806810161412
iteration: 197 lik: -326.3275610610435
iteration: 198 lik: -326.0774576166398
iteration: 199 lik: -325.8303280629349
Norm of the learned weights = 26.19167343409232
Length of the weight vector = 1364
-----------------Printing train set performance-----------------
Accuracy=0.9789190401435299
P, R, and F1 score of the positive class=0.8855799373040752 0.9641638225255973 0.9232026143790849
P, R, and F1 score of the negative class=0.9945040565297043 0.981151562096566 0.9877826878086821
Confusion Matrix
565	21
73	3800
-----------------Printing test set performance-----------------
Accuracy=0.957847533632287
P, R, and F1 score of the positive class=0.8166666666666667 0.9130434782608695 0.8621700879765396
P, R, and F1 score of the negative class=0.9850267379679144 0.9654088050314465 0.9751191106405505
Confusion Matrix
147	14
33	921
